# data file for the Fltk User Interface Designer (fluid)
version 1.0109 
header_name {.h} 
code_name {.cpp}
comment {//
// FannTool, GUI tool for ANN by using FANN library
// Programmed by BlueKid
// http://derindelimavi.blogspot.com/
// Send me any suggestion, modification or bugs. 
// Don't hesitate to contact  me for any question, 
// I will be very grateful with your feedbacks.
// bluekid70@gmail.com
// Copyright (C) 2008  BlueKid
//
// This program is free software; you can redistribute it and/or
// modify it under the terms of the GNU General Public License
// as published by the Free Software Foundation; either version 2
// of the License, or  any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program; if not, write to the Free Software
// Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
//
} {in_source in_header
} 

decl {\#include <string>} {public
} 

decl {\#include <math.h>} {public
} 

decl {\#include <deque>} {public
} 

decl {\#include "Fl_PlotXY.H"} {public
} 

decl {\#include "DataProcess.h"} {public
} 

decl {\#include <FL/fl_ask.H>} {public
} 

decl {\#include <FL/Fl_Float_Input.H>} {public
} 

decl {\#include <FL/Fl_File_Chooser.H>} {public
} 

decl {\#include <FL/Fl_Chart.H>} {public
} 

decl {\#include <stdio.h>} {public
} 

decl {\#include <fann.h>} {public
} 

decl {int Act[13]={
	  FANN_LINEAR,
	  FANN_SIGMOID,
	  FANN_SIGMOID_STEPWISE,
	  FANN_SIGMOID_SYMMETRIC,
	  FANN_SIGMOID_SYMMETRIC_STEPWISE,
	  FANN_GAUSSIAN,
	  FANN_GAUSSIAN_SYMMETRIC,
	  FANN_ELLIOT,
	  FANN_ELLIOT_SYMMETRIC,
	  FANN_LINEAR_PIECE,
	  FANN_LINEAR_PIECE_SYMMETRIC,
	  FANN_SIN_SYMMETRIC,
	  FANN_COS_SYMMETRIC 
    };} {} 

decl {int Line;} {} 

comment {MinMSE and MinANN
Min Training MSE
Min Testing MSE
Min OPS MSE
Latest MSE
both Training and Testing MSE
} {in_source not_in_header
} 

decl {struct fann* MinANN[4];} {} 

decl {double MinTrainingMSE[4];} {} 

decl {double MinTestingMSE[4];} {} 

decl {FannTool ft;} {} 

decl {bool cascadeFirst;} {} 

decl {DataProcess *rdp;} {} 

decl {TimeSeri *tdp;} {} 

decl {DataItem *itmp;} {} 

decl {std::deque<float> cX;} {} 

decl {std::deque<float> cY;} {} 

Function {} {} {
  Fl_Window window_main {
    label {FANN Tool 1.0} open
    xywh {-32000 -32000 776 711} type Double box PLASTIC_DOWN_BOX color 21 labelcolor 1 align 80 hide
    code0 {Fl::visual(FL_DOUBLE|FL_INDEX);}
    code1 {if((window_main->w()> Fl::w())||(window_main->h()> Fl::h())){ window_main->resizable(window_main); window_main->resize(window_main->x(),window_main->y(),Fl::w(),Fl::h());}}
  } {
    Fl_Menu_Bar {} {
      xywh {0 0 777 35} box GTK_UP_BOX color 29
    } {
      Submenu {} {
        label File
        xywh {0 0 62 20}
      } {
        MenuItem {} {
          label {Open Train Data}
          callback {ft.LoadTrainData();}
          xywh {0 0 30 20}
        }
        MenuItem {} {
          label {Open Test Data}
          callback {ft.LoadTestData();}
          xywh {10 10 30 20}
        }
        MenuItem {} {
          label {Open Log}
          callback {ft.LoadLog();}
          xywh {0 0 30 20}
        }
        MenuItem {} {
          label {Save Log}
          callback {ft.SaveLog();}
          xywh {10 10 30 20}
        }
        MenuItem {} {
          label {Clear Log}
          callback {ft.ClearLog();}
          xywh {20 20 30 20}
        }
        MenuItem {} {
          label Exit
          callback {exit(0);}
          xywh {30 30 30 20}
        }
      }
      Submenu {} {
        label {Neural Network} open
        xywh {0 0 62 20}
      } {
        Submenu {} {
          label Detect
          xywh {0 0 62 20}
        } {
          MenuItem {} {
            label {Optimum Training Algorithm}
            callback {ft.OptimumAlgorithm();}
            xywh {10 10 30 20}
          }
          MenuItem {} {
            label {Optimum Activation Functions}
            callback {ft.OptimumActivations();}
            xywh {20 20 30 20}
          }
        }
        Submenu {} {
          label Train open
          xywh {0 0 62 20}
        } {
          MenuItem {} {
            label Normal
            callback {ft.TrainNormal();}
            xywh {0 0 30 20}
          }
          MenuItem {} {
            label Cascade
            callback {ft.TrainCascade();}
            xywh {10 10 30 20}
          }
        }
        MenuItem {} {
          label Test
          callback {ft.Test();}
          xywh {10 10 30 20}
        }
        Submenu {} {
          label Run
          xywh {10 10 62 20}
        } {
          MenuItem {} {
            label Normal
            callback {ft.RunNormal();}
            xywh {10 10 30 20}
          }
          MenuItem {} {
            label {with File}
            callback {ft.RunwithFile();}
            xywh {20 20 30 20}
          }
        }
        MenuItem {} {
          label Info
          callback {ft.NeuralNetworkInfo();}
          xywh {20 20 30 20}
        }
      }
      MenuItem {} {
        label {Data Processing}
        callback {ft.DataProcessing();}
        xywh {0 0 30 20}
      }
      Submenu {} {
        label Help
        xywh {5 5 62 20}
      } {
        MenuItem {} {
          label {About FannTool}
          callback {ft.About();}
          xywh {0 0 30 20}
        }
      }
    }
    Fl_Group {} {
      xywh {5 91 765 229}
    } {
      Fl_Group {} {
        xywh {145 106 570 60}
      } {
        Fl_Input DataFile {
          label {Training Data File }
          xywh {145 108 420 23}
        }
        Fl_Input TestFile {
          label {Testing Data File }
          xywh {145 143 420 23}
        }
        Fl_Repeat_Button {} {
          label {...}
          callback {// GetTrainData
ft.LoadTrainData();}
          xywh {575 106 20 25} box PLASTIC_UP_BOX color 4 labeltype ENGRAVED_LABEL
        }
        Fl_Repeat_Button {} {
          label {...}
          callback {// GetTestData
ft.LoadTestData();}
          xywh {575 141 20 25} box PLASTIC_UP_BOX color 4 labeltype ENGRAVED_LABEL
        }
        Fl_Value_Output Input {
          label {Input :}
          xywh {665 107 50 24}
        }
        Fl_Value_Output Output {
          label {Output  :}
          xywh {665 142 50 24}
        }
      }
      Fl_Group {} {open
        xywh {293 181 477 129}
      } {
        Fl_Spinner Layer {
          label {\# of Layer}
          callback {if(Layer->value()==3){
 Hid2->deactivate();
 Hid3->deactivate(); 
}
else if(Layer->value()==4){
 Hid2->activate();
 Hid3->deactivate(); 
}
else if(Layer->value()==5){
 Hid2->activate();
 Hid3->activate();
} 
SetHiddens();}
          xywh {293 182 37 24} color 7 when 1 value 1
          code0 {o->value(3);}
          code1 {o->maximum(5);}
          code2 {o->minimum(3);}
        }
        Fl_Spinner Hid1 {
          label {Hid.  Layer 1 :}
          xywh {432 182 45 24} value 1
          code0 {o->value(0);}
        }
        Fl_Spinner Hid2 {
          label {Hid. Layer 2 :}
          xywh {575 182 45 24} value 1 deactivate
          code0 {o->value(0);}
        }
        Fl_Spinner Hid3 {
          label {Hid. Layer 3 :}
          xywh {715 182 45 24} value 1 deactivate
          code0 {o->value(0);}
        }
        Fl_Choice Method {
          label {Training Method :} open
          xywh {430 215 320 25} down_box BORDER_BOX
          code0 {FillMethods();}
          code1 {o->value(2);}
        } {}
        Fl_Choice HiddenActivationF {
          label {Activation Function Hidden :} open
          xywh {430 250 320 25} down_box BORDER_BOX
        } {}
        Fl_Choice OutputActivationF {
          label {Activation Function Output :} open
          xywh {430 285 320 25} down_box BORDER_BOX
          code0 {FillActivationF();}
          code1 {o->value(3);}
          code2 {HiddenActivationF->value(3);}
        } {}
      }
      Fl_Group {} {open
        xywh {10 176 205 144} box EMBOSSED_FRAME align 0
      } {
        Fl_Choice StopFunction {
          label {Stop Function} open
          xywh {20 196 190 22} down_box BORDER_BOX align 5
          code0 {StopFunction->add("FANN_STOPFUNC_MSE");}
          code1 {StopFunction->add("FANN_STOPFUNC_BIT");}
          code2 {StopFunction->value(0);}
        } {}
        Fl_Value_Input EReports {
          label {Epoch Between Reports}
          xywh {20 242 55 23} align 5 minimum 1 maximum 50000 step 10 value 1000
        }
        Fl_Value_Input MaxEpoch {
          label {Max Epoch : }
          callback {/*
if(MaxEpoch->value()>=1000)
  MaxEpoch->step(100);  
if(MaxEpoch->value()>=10000)
  MaxEpoch->step(1000);  
if(MaxEpoch->value()>=100000)
  MaxEpoch->step(10000);
  
*/}
          xywh {20 286 60 24} align 5 minimum 1000 maximum 1e+006 step 1000 value 500000
        }
      }
    }
    Fl_Tabs Tabs {open
      xywh {0 324 776 381} box GTK_UP_FRAME color 21
      code0 {o->value(LogG);}
    } {
      Fl_Group LogG {
        label Log
        xywh {6 370 755 335}
      } {
        Fl_Browser Out {
          xywh {6 371 750 324} type Hold box GTK_DOWN_BOX color 31 labelcolor 2
        }
      }
      Fl_Group GraphG {
        label Graphic open
        xywh {1 365 775 340} hide
      } {
        Fl_Box Graph {
          tooltip {MSE vs Epoch} xywh {11 371 746 294} box GTK_DOWN_BOX color 16
          code0 {Line=Graph->newline();}
          code1 {Graph->scalemode(Line,FL_PLOTXY_AUTO);}
          code2 {Graph->linecolor(Line,FL_BLUE);}
          class Fl_PlotXY
        }
        Fl_Value_Output Ep {
          label {Epoch :}
          xywh {63 671 60 24} labelcolor 4
        }
        Fl_Value_Output Mse {
          label {MSE :}
          xywh {174 671 110 24} labelcolor 4
        }
        Fl_Spinner nVData {
          label {\# of Visible Data :}
          xywh {712 671 48 24} labelcolor 4 minimum 20 maximum 500 value 500
        }
        Fl_Value_Output BitFailOut {
          label {Bit Fail  :}
          xywh {549 671 44 24} labelcolor 4
        }
        Fl_Value_Output TestMse {
          label {Test MSE :}
          tooltip {Test Data MSE} xywh {375 671 110 24} labelcolor 1 deactivate
        }
      }
      Fl_Group {} {
        label {Fine Tuning}
        xywh {5 365 755 335} hide
      } {
        Fl_Value_Input DesiredError {
          label {Desired Error ( MSE ) :}
          xywh {164 396 72 24} maximum 0.1 step 1e-005 value 0.0001
        }
        Fl_Value_Input BitFail {
          label {Bit Fail Limit :}
          xywh {165 434 72 24} maximum 0.1 step 1e-005 value 0.035
        }
        Fl_Choice ErrorFunction {
          label {Error Function} open
          xywh {326 393 225 24} down_box BORDER_BOX align 5
          code0 {ErrorFunction->add("FANN_ERRORFUNC_LINEAR");}
          code1 {ErrorFunction->add("FANN_ERRORFUNC_TANH");}
          code2 {ErrorFunction->value(0);}
        } {}
        Fl_Value_Input HiddenStepness {
          label {Hidden Activation Stepness  }
          xywh {330 438 67 22} align 5 maximum 0 value 0.5
        }
        Fl_Value_Input OutputStepness {
          label {Output Activation Stepness  }
          xywh {330 486 67 24} align 5 maximum 0 value 0.5
        }
        Fl_Value_Input DecayFactor {
          label {Quickprop Decay Factor}
          xywh {579 393 67 24} align 5 minimum -0.2 maximum 0 value -0.0001
        }
        Fl_Value_Input MuFactor {
          label {Quickprop Mu Factor}
          xywh {579 436 70 24} align 5 minimum 1 maximum 3 value 1.75
        }
        Fl_Value_Input IncreaseFactor {
          label {RPROP Increase Factor }
          xywh {579 486 70 24} align 5 minimum 1 maximum 3 value 1.2
        }
        Fl_Value_Input DecreaseFactor {
          label {RPROP Decrease Factor }
          xywh {579 536 70 24} align 5 value 0.5
        }
        Fl_Value_Input DeltaMin {
          label {RPROP Minimum Step-Size}
          xywh {579 591 70 24} align 5 maximum 3
        }
        Fl_Value_Input DeltaMax {
          label {RPROP Maximum Step-Size}
          xywh {579 641 70 24} align 5 minimum 1 maximum 100 value 50
        }
        Fl_Counter ConnectionRate {
          label {Connection Rate}
          xywh {130 484 142 24} align 4 minimum 0.1 maximum 1 value 1
        }
        Fl_Counter LearningRate {
          label {Learning Rate}
          xywh {130 521 143 25} align 4 minimum 0.1 maximum 1 value 0.7
        }
        Fl_Counter Momentum {
          label {Momentum :}
          xywh {410 521 140 25} align 4 minimum 0 maximum 1
        }
        Fl_Check_Button Shuffle {
          label {Shuffle Train Data}
          xywh {448 551 26 25} down_box DOWN_BOX align 4
        }
        Fl_Check_Button InitWghts {
          label {Initialize the weights ( Widrow + Nguyen Alg.)}
          xywh {295 552 19 24} down_box DOWN_BOX align 4
        }
        Fl_Check_Button OverTraining {
          label {Overtraining Caution System}
          callback {if(OverTraining->value()){
  ft.overtraining=true;
  TestMse->activate();
}
else{
  ft.overtraining=false;
  TestMse->value(0);
  TestMse->deactivate();
  
}}
          xywh {295 586 19 24} down_box DOWN_BOX align 4 deactivate
        }
      }
      Fl_Group {} {
        label {Cascade Tuning}
        xywh {7 370 750 335} hide
      } {
        Fl_Value_Input OutputChange {
          label {Output Change Fraction:}
          xywh {51 476 93 24} align 5 value 0.01
        }
        Fl_Value_Input OutputStag {
          label {Output Stagnation Epochs}
          xywh {51 526 93 24} align 5 maximum 0 value 12
        }
        Fl_Value_Input CandidateChange {
          label {Candidate Change Fraction:}
          xywh {51 576 93 24} align 5 value 0.01
        }
        Fl_Value_Input CandidateStag {
          label {Candidate Stagnation Epochs}
          xywh {51 626 93 24} align 5 maximum 0 value 12
        }
        Fl_Value_Input WeighMultiplier {
          label {Weight Multiplier}
          xywh {301 421 93 24} align 5 value 0.4
        }
        Fl_Value_Input CandidateLimit {
          label {Candidate Limit}
          xywh {301 471 93 24} align 5 maximum 2000 value 1000
        }
        Fl_Value_Input MaxOutEpoch {
          label {Maximum Out Epochs}
          xywh {301 521 93 24} align 5 maximum 500 value 150
        }
        Fl_Value_Input MaxCasndidatetEpoch {
          label {Maximum Candidate Epochs}
          xywh {301 571 93 24} align 5 maximum 500 value 150
        }
        Fl_Value_Input NumCandidateGroups {
          label {Number of Candidate Groups}
          xywh {301 616 93 24} align 5 maximum 0 value 2
        }
        Fl_Value_Input MaxCascade {
          label {Maximum Number of Neurons}
          xywh {51 426 93 24} align 5 minimum 1 maximum 0 value 10
        }
      }
    }
    Fl_Group {} {
      xywh {10 42 494 56} box EMBOSSED_FRAME
    } {
      Fl_Button But_Alg {
        callback {ft.OptimumAlgorithm();}
        tooltip {Detect Optimum Training Algorithm} image {img/alg.png} xywh {20 56 32 32} box PLASTIC_UP_BOX down_box FLAT_BOX color 4 align 128 deactivate
        code0 {InactImg(o);}
      }
      Fl_Button But_Act {
        callback {ft.OptimumActivations();}
        tooltip {Detect Optimum Activation Functions} image {img/act.png} xywh {60 56 32 32} box PLASTIC_UP_BOX color 4 align 128 deactivate
        code0 {InactImg(o);}
      }
      Fl_Menu_Button But_Trn {
        callback {if(But_Trn->value()==0)
  ft.TrainNormal();
else if(But_Trn->value()==1)
  ft.TrainCascade();} open
        tooltip {Train ANN} image {img/train.png} xywh {100 56 32 32} box PLASTIC_UP_BOX color 4 deactivate
        code0 {o->add("Normal");}
        code1 {o->add("Cascade");}
        code2 {InactImg(o);}
      } {}
      Fl_Button But_Test {
        callback {ft.Test();}
        tooltip {Test ANN} image {img/test.png} xywh {140 56 32 32} box PLASTIC_UP_BOX color 4 align 128 deactivate
        code0 {InactImg(o);}
      }
      Fl_Menu_Button But_Run {
        callback {if(But_Run->value()==0)
  ft.RunNormal();
else if(But_Run->value()==1)
  ft.RunwithFile();} open
        tooltip {Run ANN} image {img/Run.png} xywh {180 56 32 32} box PLASTIC_UP_BOX color 4
        code0 {o->add("Normal");}
        code1 {o->add("with File");}
        code2 {InactImg(o);}
      } {}
      Fl_Button {} {
        callback {ft.ClearLog();}
        tooltip {Clear Log} image {img/clear.png} xywh {220 56 32 32} box PLASTIC_UP_BOX color 4 align 128
        code0 {InactImg(o);}
      }
      Fl_Button {} {
        callback {ft.SaveLog();}
        tooltip {Save Log} image {img/save-2.png} xywh {260 56 32 32} box PLASTIC_UP_BOX color 4 align 128
        code0 {InactImg(o);}
      }
      Fl_Button Stop {
        callback {ft.StopProcess();}
        tooltip {Stop Process} image {img/stop.png} xywh {300 56 32 32} box PLASTIC_UP_BOX color 4 labelcolor 1 align 128 deactivate
        code0 {InactImg(o);}
      }
      Fl_Button {} {
        callback {ft.LoadLog();}
        tooltip {Load  Log} image {img/load.png} xywh {340 56 32 32} box PLASTIC_UP_BOX color 4 align 128
        code0 {InactImg(o);}
      }
      Fl_Button {} {
        callback {ft.About();}
        tooltip Help image {img/Help.png} xywh {380 56 32 32} box PLASTIC_UP_BOX color 4 align 128
        code0 {InactImg(o);}
      }
      Fl_Button {} {
        callback {ft.DataProcessing();}
        tooltip {Data Processing} image {img/Data.png} xywh {420 56 32 32} box PLASTIC_UP_BOX color 4 align 128
        code0 {InactImg(o);}
      }
      Fl_Button {} {
        callback {ft.NeuralNetworkInfo();}
        tooltip {Neural Network Information} image {img/Information.png} xywh {462 56 32 32} box PLASTIC_UP_BOX color 4 align 128
        code0 {InactImg(o);}
      }
    }
  }
} 

Function {cb_Ok(Fl_Return_Button*o, void*w)} {} {
  code {((Fl_Window *)(o->parent()))->hide();} {}
} 

Function {Refresh()} {return_type {inline void}
} {
  code {Out->bottomline(Out->size());
Out->redraw();
Graph->redraw();
Ep->redraw();
Mse->redraw();
Fl::wait(0);} {}
} 

Function {FillMethods()} {} {
  code {/*
FANN_TRAIN_INCREMENTAL = 0,
FANN_TRAIN_BATCH,
FANN_TRAIN_RPROP,
FANN_TRAIN_QUICKPROP
*/	
	
Method->add("FANN_TRAIN_INCREMENTAL");
Method->add("FANN_TRAIN_BATCH");
Method->add("FANN_TRAIN_RPROP");
Method->add("FANN_TRAIN_QUICKPROP");} {}
} 

Function {FillActivationF()} {} {
  code {char *ActF[13]={
	  "FANN_LINEAR",
	  "FANN_SIGMOID",
	  "FANN_SIGMOID_STEPWISE",
	  "FANN_SIGMOID_SYMMETRIC",
	  "FANN_SIGMOID_SYMMETRIC_STEPWISE",
	  "FANN_GAUSSIAN",
	  "FANN_GAUSSIAN_SYMMETRIC",
	  "FANN_ELLIOT",
	  "FANN_ELLIOT_SYMMETRIC",
	  "FANN_LINEAR_PIECE",
	  "FANN_LINEAR_PIECE_SYMMETRIC",
	  "FANN_SIN_SYMMETRIC",
	  "FANN_COS_SYMMETRIC" 

    };
    
    
    for(int i=0;i<13;i++){
      HiddenActivationF->add(ActF[i]);
      OutputActivationF->add(ActF[i]);      
    }} {}
} 

Function {fann_copy(const struct fann* orig)} {return_type {struct fann*}
} {
  code {/* deep copy of the fann structure */
/* adapted from FANN CVS */

    struct fann* copy;
    unsigned int num_layers = orig->last_layer - orig->first_layer;
    struct fann_layer *orig_layer_it, *copy_layer_it;
    unsigned int layer_size;
    struct fann_neuron *last_neuron,*orig_neuron_it,*copy_neuron_it;
    unsigned int i;
    struct fann_neuron *orig_first_neuron,*copy_first_neuron;
    unsigned int input_neuron;

    copy = fann_allocate_structure(num_layers);
    if (copy==NULL) {
        fann_error((struct fann_error*)orig, FANN_E_CANT_ALLOCATE_MEM);
        return NULL;
    }
    copy->errno_f = orig->errno_f;
    if (orig->errstr)
    {
        copy->errstr = (char *) malloc(FANN_ERRSTR_MAX);
        if (copy->errstr == NULL)
        {
            fann_destroy(copy);
            return NULL;
        }
        strcpy(copy->errstr,orig->errstr);
    }
    copy->error_log = orig->error_log;

    copy->learning_rate = orig->learning_rate;
    copy->learning_momentum = orig->learning_momentum;
    copy->connection_rate = orig->connection_rate;
    copy->network_type = orig->network_type;
    copy->num_MSE = orig->num_MSE;
    copy->MSE_value = orig->MSE_value;
    copy->num_bit_fail = orig->num_bit_fail;
    copy->bit_fail_limit = orig->bit_fail_limit;
    copy->train_error_function = orig->train_error_function;
    copy->train_stop_function = orig->train_stop_function;
    copy->callback = orig->callback;
    copy->cascade_output_change_fraction = orig->cascade_output_change_fraction;
    copy->cascade_output_stagnation_epochs = orig->cascade_output_stagnation_epochs;
    copy->cascade_candidate_change_fraction = orig->cascade_candidate_change_fraction;
    copy->cascade_candidate_stagnation_epochs = orig->cascade_candidate_stagnation_epochs;
    copy->cascade_best_candidate = orig->cascade_best_candidate;
    copy->cascade_candidate_limit = orig->cascade_candidate_limit;
    copy->cascade_weight_multiplier = orig->cascade_weight_multiplier;
    copy->cascade_max_out_epochs = orig->cascade_max_out_epochs;
    copy->cascade_max_cand_epochs = orig->cascade_max_cand_epochs;
	copy->user_data = orig->user_data;

   /* copy cascade activation functions */
    copy->cascade_activation_functions_count = orig->cascade_activation_functions_count;
    copy->cascade_activation_functions = (enum fann_activationfunc_enum *)realloc(copy->cascade_activation_functions,
        copy->cascade_activation_functions_count * sizeof(enum fann_activationfunc_enum));
    if(copy->cascade_activation_functions == NULL)
    {
        fann_error((struct fann_error*)orig, FANN_E_CANT_ALLOCATE_MEM);
        fann_destroy(copy);
        return NULL;
    }
    memcpy(copy->cascade_activation_functions,orig->cascade_activation_functions,
            copy->cascade_activation_functions_count * sizeof(enum fann_activationfunc_enum));

    /* copy cascade activation steepnesses */
    copy->cascade_activation_steepnesses_count = orig->cascade_activation_steepnesses_count;
    copy->cascade_activation_steepnesses = (fann_type *)realloc(copy->cascade_activation_steepnesses, copy->cascade_activation_steepnesses_count * sizeof(fann_type));
    if(copy->cascade_activation_steepnesses == NULL)
    {
        fann_error((struct fann_error*)orig, FANN_E_CANT_ALLOCATE_MEM);
        fann_destroy(copy);
        return NULL;
    }
    memcpy(copy->cascade_activation_steepnesses,orig->cascade_activation_steepnesses,copy->cascade_activation_steepnesses_count * sizeof(fann_type));

    copy->cascade_num_candidate_groups = orig->cascade_num_candidate_groups;

    /* copy candidate scores, if used */
    if (orig->cascade_candidate_scores == NULL)
    {
        copy->cascade_candidate_scores = NULL;
    }
    else
    {
        copy->cascade_candidate_scores =
            (fann_type *) malloc(fann_get_cascade_num_candidates(copy) * sizeof(fann_type));
        if(copy->cascade_candidate_scores == NULL)
        {
            fann_error((struct fann_error *) orig, FANN_E_CANT_ALLOCATE_MEM);
            fann_destroy(copy);
            return NULL;
        }
        memcpy(copy->cascade_candidate_scores,orig->cascade_candidate_scores,fann_get_cascade_num_candidates(copy) * sizeof(fann_type));
    }

    copy->quickprop_decay = orig->quickprop_decay;
    copy->quickprop_mu = orig->quickprop_mu;
    copy->rprop_increase_factor = orig->rprop_increase_factor;
    copy->rprop_decrease_factor = orig->rprop_decrease_factor;
    copy->rprop_delta_min = orig->rprop_delta_min;
    copy->rprop_delta_max = orig->rprop_delta_max;
    copy->rprop_delta_zero = orig->rprop_delta_zero;

    /* user_data is not deep copied.  user should use fann_copy_with_user_data() for that */
    copy->user_data = orig->user_data;

\#ifdef FIXEDFANN
    copy->decimal_point = orig->decimal_point;
    copy->multiplier = orig->multiplier;
    memcpy(copy->sigmoid_results,orig->sigmoid_results,6*sizeof(fann_type));
    memcpy(copy->sigmoid_values,orig->sigmoid_values,6*sizeof(fann_type));
    memcpy(copy->sigmoid_symmetric_results,orig->sigmoid_symmetric_results,6*sizeof(fann_type));
    memcpy(copy->sigmoid_symmetric_values,orig->sigmoid_symmetric_values,6*sizeof(fann_type));
\#endif


    /* copy layer sizes, prepare for fann_allocate_neurons */
    for (orig_layer_it = orig->first_layer, copy_layer_it = copy->first_layer;
            orig_layer_it != orig->last_layer; orig_layer_it++, copy_layer_it++)
    {
        layer_size = orig_layer_it->last_neuron - orig_layer_it->first_neuron;
        copy_layer_it->first_neuron = NULL;
        copy_layer_it->last_neuron = copy_layer_it->first_neuron + layer_size;
        copy->total_neurons += layer_size;
    }
    copy->num_input = orig->num_input;
    copy->num_output = orig->num_output;


    /* copy scale parameters, when used */
\#ifndef FIXEDFANN
    if (orig->scale_mean_in != NULL)
    {
        fann_allocate_scale(copy);
        for (i=0; i < orig->num_input ; i++) {
            copy->scale_mean_in[i] = orig->scale_mean_in[i];
            copy->scale_deviation_in[i] = orig->scale_deviation_in[i];
            copy->scale_new_min_in[i] = orig->scale_new_min_in[i];
            copy->scale_factor_in[i] = orig->scale_factor_in[i];
        }
        for (i=0; i < orig->num_output ; i++) {
            copy->scale_mean_out[i] = orig->scale_mean_out[i];
            copy->scale_deviation_out[i] = orig->scale_deviation_out[i];
            copy->scale_new_min_out[i] = orig->scale_new_min_out[i];
            copy->scale_factor_out[i] = orig->scale_factor_out[i];
        }
    }
\#endif

    /* copy the neurons */
    fann_allocate_neurons(copy);
    if (copy->errno_f == FANN_E_CANT_ALLOCATE_MEM)
    {
        fann_destroy(copy);
        return NULL;
    }
    layer_size = (orig->last_layer-1)->last_neuron - (orig->last_layer-1)->first_neuron;
    memcpy(copy->output,orig->output, layer_size * sizeof(fann_type));

    last_neuron = (orig->last_layer - 1)->last_neuron;
    for (orig_neuron_it = orig->first_layer->first_neuron, copy_neuron_it = copy->first_layer->first_neuron;
            orig_neuron_it != last_neuron; orig_neuron_it++, copy_neuron_it++)
    {
        memcpy(copy_neuron_it,orig_neuron_it,sizeof(struct fann_neuron));
    }
 /* copy the connections */
    copy->total_connections = orig->total_connections;
    fann_allocate_connections(copy);
    if (copy->errno_f == FANN_E_CANT_ALLOCATE_MEM)
    {
        fann_destroy(copy);
        return NULL;
    }

    orig_first_neuron = orig->first_layer->first_neuron;
    copy_first_neuron = copy->first_layer->first_neuron;
    for (i=0; i < orig->total_connections; i++)
    {
        copy->weights[i] = orig->weights[i];
        input_neuron = orig->connections[i] - orig_first_neuron;
        copy->connections[i] = copy_first_neuron + input_neuron;
    }

    if (orig->train_slopes)
    {
        copy->train_slopes = (fann_type *) malloc(copy->total_connections_allocated * sizeof(fann_type));
        if (copy->train_slopes == NULL)
        {
            fann_error((struct fann_error *) orig, FANN_E_CANT_ALLOCATE_MEM);
            fann_destroy(copy);
            return NULL;
        }
        memcpy(copy->train_slopes,orig->train_slopes,copy->total_connections_allocated * sizeof(fann_type));
    }

    if (orig->prev_steps)
    {
        copy->prev_steps = (fann_type *) malloc(copy->total_connections_allocated * sizeof(fann_type));
        if (copy->prev_steps == NULL)
        {
            fann_error((struct fann_error *) orig, FANN_E_CANT_ALLOCATE_MEM);
            fann_destroy(copy);
            return NULL;
        }
        memcpy(copy->prev_steps, orig->prev_steps, copy->total_connections_allocated * sizeof(fann_type));
    }

    if (orig->prev_train_slopes)
    {
        copy->prev_train_slopes = (fann_type *) malloc(copy->total_connections_allocated * sizeof(fann_type));
        if (copy->prev_train_slopes == NULL)
        {
            fann_error((struct fann_error *) orig, FANN_E_CANT_ALLOCATE_MEM);
            fann_destroy(copy);
            return NULL;
        }
        memcpy(copy->prev_train_slopes,orig->prev_train_slopes, copy->total_connections_allocated * sizeof(fann_type));
    }

    if (orig->prev_weights_deltas)
    {
        copy->prev_weights_deltas = (fann_type *) malloc(copy->total_connections_allocated * sizeof(fann_type));
        if(copy->prev_weights_deltas == NULL)
        {
            fann_error((struct fann_error *) orig, FANN_E_CANT_ALLOCATE_MEM);
            fann_destroy(copy);
            return NULL;
        }
        memcpy(copy->prev_weights_deltas, orig->prev_weights_deltas,copy->total_connections_allocated * sizeof(fann_type));
    }

    return copy;} {}
} 

Function {GetWeigths(struct fann *ann)} {return_type {fann_type *}
} {
  code {fann_type *w;
w=(fann_type *)malloc(ann->total_connections*sizeof(fann_type));
for (unsigned int i=0; i < ann->total_connections; i++)
   w[i] = ann->weights[i];
return w;} {}
} 

Function {SetWeights(struct fann *ann,fann_type *w)} {return_type void
} {
  code {for (unsigned int i=0; i < ann->total_connections; i++)
      ann->weights[i]=w[i];} {}
} 

Function {ExamineTrain(struct fann *ann,fann_train_enum tal,fann_activationfunc_enum hact,fann_activationfunc_enum oact,fann_train_data *TrainData)} {return_type fann_type
} {
  code {fann_set_training_algorithm(ann,tal);
fann_set_activation_function_hidden(ann, hact);
fann_set_activation_function_output(ann, oact);
fann_set_callback(ann, LogOut );
// inith Graph
cX.clear();
cY.clear();
//
fann_train_on_data(ann, TrainData, 2000, 250, 0.0);

  double trainMSE=fann_get_MSE(ann);
  double testMSE=-1;
  if(ft.TestData && ft.overtraining){
    fann_reset_MSE(ann);
    fann_test_data(ann,ft.TestData);
    testMSE=fann_get_MSE(ann);
    return (trainMSE+testMSE)/2;
    
  }
  else
    return trainMSE;} {}
} 

Function {LogOut(struct fann *ann, struct fann_train_data *train,unsigned int max_epochs, unsigned int epochs_between_reports,float desired_error, unsigned int epochs)} {return_type {int FANN_API}
} {
  code {char Buf[512];
  double trainMSE=fann_get_MSE(ann);
  double testMSE=-1;
  unsigned int newBitFail=fann_get_bit_fail(ann);
  if(ft.TestData && ft.overtraining){
    fann_reset_MSE(ann);
    fann_test_data(ann,ft.TestData);
    testMSE=fann_get_MSE(ann);
    fann_test_data(ann,ft.TrainData);
    sprintf(Buf,"%08d : %f       : %f       : %d ", epochs,trainMSE ,testMSE,newBitFail);
  }
  else
    sprintf(Buf,"%08d : %f       : %d ", epochs,trainMSE , newBitFail);
  Out->add(Buf);  
  // Memorizing Begin
     if(epochs==1){
       for(int i=0;i<3;i++) {
         MinTrainingMSE[i]=trainMSE;
         if(ft.TestData && ft.overtraining)
            MinTestingMSE[i]=testMSE;
       }
       MinANN[3]=ann;
       
     }

     // Latest 
     MinTrainingMSE[3]= trainMSE;
     MinTestingMSE[3]= testMSE; 
//     if( MinANN[3]) fann_destroy( MinANN[3]);
//       MinANN[3]=fann_copy(ann);
  
     // Minimum Training MSE
     if(MinTrainingMSE[0]> trainMSE ){
       if( MinANN[0]) fann_destroy( MinANN[0]);
       MinANN[0]=fann_copy(ann);
       MinTrainingMSE[0]=trainMSE;
       if(ft.TestData && ft.overtraining)
           MinTestingMSE[0]=testMSE;
       
     }
     if(ft.TestData && ft.overtraining){
       // Minimum Testing MSE
       if(MinTestingMSE[1]> testMSE ){
         if( MinANN[1]) fann_destroy( MinANN[1]);
         MinANN[1]=fann_copy(ann);
         MinTrainingMSE[1]=trainMSE;
         MinTestingMSE[1]=testMSE;

       }
       // Minimum (Training MSE + Testing MSE )/2 
       if((MinTestingMSE[2]+ MinTrainingMSE[2])> (trainMSE + testMSE) ){
         if( MinANN[2]) fann_destroy( MinANN[2]);
         MinANN[2]=fann_copy(ann);
         MinTrainingMSE[2]=trainMSE;
         MinTestingMSE[2]=testMSE;

       }       
       
         
     }     
  
    // Memorizing End

  
  DrawGraph(epochs,trainMSE,testMSE,newBitFail);  
  Refresh();
  if(ft.stop)
    return -1;
  return 1;} {}
} 

Function {CascadeLogOut(struct fann *ann, struct fann_train_data *train,unsigned int max_epochs, unsigned int epochs_between_reports,float desired_error, unsigned int epochs)} {return_type {int FANN_API}
} {
  code {char Buf[512];

double trainMSE=fann_get_MSE(ann);
double testMSE=-1;
unsigned int newBitFail=fann_get_bit_fail(ann);

if(ft.TestData && ft.overtraining){
    fann_reset_MSE(ann);
    fann_test_data(ann,ft.TestData);
    testMSE=fann_get_MSE(ann);
    fann_test_data(ann,ft.TrainData);
    sprintf(Buf,"%08d : %f       : %f       : %d ", epochs,trainMSE ,testMSE ,newBitFail);   
}
else
  sprintf(Buf,"%08d : %f       : %d  ", epochs,trainMSE , newBitFail);
/*
sprintf(Buf,"Current error: %.6f. Total error:%8.4f. Epochs %5d. Bit fail %3d",
					 newMSE, ann->MSE_value, epochs, ann->num_bit_fail);
					 
	*/
					 
Out->add(Buf);					 
					 
if((ann->last_layer-2) != ann->first_layer)
{
  sprintf(Buf,"@C4Candidate steepness %.2f. function %s", 
     (ann->last_layer-2)->first_neuron->activation_steepness,
     FANN_ACTIVATIONFUNC_NAMES[(ann->last_layer-2)->first_neuron->activation_function]);
  Out->add(Buf);     
}


  // Memorizing Begin
     if(cascadeFirst){
       for(int i=0;i<3;i++) {
         MinTrainingMSE[i]=trainMSE;
         if(ft.TestData && ft.overtraining)
            MinTestingMSE[i]=testMSE;
       }
       MinANN[3]=ann;
       cascadeFirst=false;
       
     }

     // Latest 
     MinTrainingMSE[3]= trainMSE;
     MinTestingMSE[3]= testMSE; 
//     if( MinANN[3]) fann_destroy( MinANN[3]);
//       MinANN[3]=fann_copy(ann);
  
     // Minimum Training MSE
     if(MinTrainingMSE[0]> trainMSE ){
       if( MinANN[0]) fann_destroy( MinANN[0]);
       MinANN[0]=fann_copy(ann);
       MinTrainingMSE[0]=trainMSE;
       if(ft.TestData && ft.overtraining)
           MinTestingMSE[0]=testMSE;
       
     }
     if(ft.TestData && ft.overtraining){
       // Minimum Testing MSE
       if(MinTestingMSE[1]> testMSE ){
         if( MinANN[1]) fann_destroy( MinANN[1]);
         MinANN[1]=fann_copy(ann);
         MinTrainingMSE[1]=trainMSE;
         MinTestingMSE[1]=testMSE;

       }
       // Minimum (Training MSE + Testing MSE )/2 
       if((MinTestingMSE[2]+ MinTrainingMSE[2])> (trainMSE + testMSE) ){       
         if( MinANN[2]) fann_destroy( MinANN[2]);
         MinANN[2]=fann_copy(ann);
         MinTrainingMSE[2]=trainMSE;
         MinTestingMSE[2]=testMSE;

       }       
       
         
     }     
  
    // Memorizing End



DrawGraph(epochs,trainMSE,testMSE,ann->num_bit_fail);

Refresh();
if(ft.stop) 
 return -1;

return 1;} {}
} 

Function {DrawGraph(float epochs, float TrainMSE, float TestMSE,unsigned int newBitFail)} {return_type {int FANN_API}
} {
  code {if(epochs < 2){
    cX.clear();
    cY.clear();  
  }

  Graph->clear(Line);

  cX.push_back(epochs);

  if(TestMSE==-1)
    cY.push_back(TrainMSE);
  else
    cY.push_back((TrainMSE+TestMSE)/2);
  

  while(cX.size()>=(int)nVData->value()){
    cX.pop_front();
    cY.pop_front();
  }

  for (int i=1; i<cX.size(); ++i) {
    Graph->add(Line,cX[i],cY[i]);
  }

  Graph->redraw();

  Ep->value(epochs);
  Mse->value(TrainMSE);
  if(TestMSE!=-1)
    TestMse->value(TestMSE);
  BitFailOut->value(newBitFail);} {}
} 

Function {SetHiddens()} {return_type void
} {
  code {int tmp,fark;

fark=(int )(Input->value()-Output->value());
if(fark > 0 ) {
  if(Layer->value()==3){
    tmp=fark/2;
    Hid1->value(tmp);
    Hid2->value(0);
    Hid3->value(0);
  }
  else if(Layer->value()==4){
    tmp=fark/3;
    Hid1->value(Input->value()-tmp);
    Hid2->value(Hid1->value()-tmp);
    Hid3->value(0);
  }
  else if(Layer->value()==5){
    tmp=fark/4;
    Hid1->value(Input->value()-tmp);
    Hid2->value(Hid1->value()-tmp);
    Hid3->value(Hid2->value()-tmp);

  }
  if(Hid1->value()==0) Hid1->value(1);
  if(Hid2->value()==0) Hid2->value(1);
  if(Hid3->value()==0) Hid3->value(1);

}
else{
 Hid1->value(1);
 Hid2->value(1);
 Hid3->value(1);
}} {}
} 

Function {InactImg(Fl_Widget* o)} {} {
  code {Fl_Image *r=o->image();
r=r->copy(r->w(),r->h());
r->inactive();
o->deimage(r);} {}
} 

class FannTool {open
} {
  decl {struct fann *ysa;} {public
  }
  decl {fann_type *result;} {public
  }
  decl {fann_type *data;} {public
  }
  decl {bool working;} {}
  decl {bool stop;} {public
  }
  decl {bool overtraining;} {public
  }
  decl {fann_train_data *TrainData} {public
  }
  decl {fann_train_data *TestData} {public
  }
  Function {FannTool()} {} {
    code {stop=false;
overtraining=false;
working=false;
TrainData=NULL;
TestData=NULL;
for(int i=0;i<4;i++){
  MinANN[i]=NULL; 
}} {}
  }
  Function {OptimumAlgorithm()} {} {
    code {if(TrainData==NULL){
  fl_alert("Firstly Load Train Data !");
  return;
}
if(working) return;
char Buf[512];
Out->clear();
ActivateStop();
if(Layer->value()==5)
  ysa = fann_create_sparse(ConnectionRate->value(),5,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Hid3->value(),(int)Output->value());
if(Layer->value()==4)
  ysa = fann_create_sparse(ConnectionRate->value(),4,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Output->value());  
else
  ysa = fann_create_sparse(ConnectionRate->value(),3,(int)Input->value(),(int)Hid1->value(),(int)Output->value());
fann_type *w=GetWeigths(ysa);
int best_ta;
fann_type min=1,mse;
for(int  ta=FANN_TRAIN_INCREMENTAL; ta<=FANN_TRAIN_QUICKPROP ;ta++) {
    SetWeights(ysa,w);
    sprintf(Buf,"@C4Train Method : %s ",FANN_TRAIN_NAMES[ta]);
    Out->add(Buf);
    Out->redraw();
    mse=ExamineTrain(ysa,(fann_train_enum)ta,FANN_SIGMOID_SYMMETRIC,FANN_SIGMOID_SYMMETRIC,TrainData);
    if(mse<min){
       min=mse;
       best_ta=ta;
    }
    if(stop) break; 
}
if(stop==false){
  sprintf(Buf,"@C1Best Train Method : %s ",FANN_TRAIN_NAMES[best_ta]);
  Method->value(best_ta);
  Out->add(Buf);
}
else
  Out->add("@C1Process Stopped !");
Out->bottomline(Out->size());
Out->redraw();
fann_destroy(ysa);
DeactivateStop();} {}
  }
  Function {OptimumActivations()} {} {
    code {if(TrainData==NULL){
  fl_alert("Firstly Load Train Data !");
  return;
}
if(working) return;
int best_ha,best_oa;
Out->clear();
ActivateStop();
if(Layer->value()==5)
  ysa = fann_create_sparse(ConnectionRate->value(),5,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Hid3->value(),(int)Output->value());
if(Layer->value()==4)
  ysa = fann_create_sparse(ConnectionRate->value(),4,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Output->value());  
else
  ysa = fann_create_sparse(ConnectionRate->value(),3,(int)Input->value(),(int)Hid1->value(),(int)Output->value());
fann_type *w=GetWeigths(ysa);

int best_ta;
fann_type min=1,mse;
char Buf[512];
for(int  i=0; i<13 && !stop;i++) {
  for(int  j=0; j<13&& !stop;j++) {
  SetWeights(ysa,w);
    sprintf(Buf,"@C4Hid Activation Func. : %s --- Out Activation Func. : %s ",FANN_ACTIVATIONFUNC_NAMES[Act[i]],FANN_ACTIVATIONFUNC_NAMES[Act[j]]);
    Out->add(Buf);
    mse=ExamineTrain(ysa,(fann_train_enum)Method->value(),(fann_activationfunc_enum) Act[i],(fann_activationfunc_enum) Act[j],TrainData);
    if(mse<min){
       min=mse;
       best_ha=i;
       best_oa=j;

    }
  }
}

if(stop==false){
  sprintf(Buf,"@C1Best  Hid. Activation Func. : %s --- Out. Activation Func. : %s ",FANN_ACTIVATIONFUNC_NAMES[Act[best_ha]],FANN_ACTIVATIONFUNC_NAMES[Act[best_oa]]);
  Out->add(Buf);
  HiddenActivationF->value(best_ha);
  OutputActivationF->value(best_oa);
} 
else
  Out->add("@C1Process Stopped !");

Out->bottomline(Out->size());
Out->redraw();
fann_destroy(ysa);
DeactivateStop();} {}
  }
  Function {TrainNormal()} {} {
    code {if(TrainData==NULL){
  fl_alert("Firstly Load Train Data !");
  return;
}
if(working) return;
char Buf[512];
Out->clear();
Fl::wait(0);    
ActivateStop();
Out->add("@C4Normal Training...");

Refresh();
 ClearTrainingMemory();
if(Layer->value()==5)
  ysa = fann_create_sparse(ConnectionRate->value(),5,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Hid3->value(),(int)Output->value());
else if(Layer->value()==4)
  ysa = fann_create_sparse(ConnectionRate->value(),4,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Output->value());  
else
  ysa = fann_create_sparse(ConnectionRate->value(),3,(int)Input->value(),(int)Hid1->value(),(int)Output->value());


fann_set_training_algorithm(ysa,(fann_train_enum)Method->value());
fann_set_activation_function_hidden(ysa,(fann_activationfunc_enum)Act[HiddenActivationF->value()]);
fann_set_activation_function_output(ysa,(fann_activationfunc_enum)Act[OutputActivationF->value()]);
fann_set_learning_rate(ysa,LearningRate->value());
fann_set_learning_momentum(ysa,Momentum->value());
fann_set_callback(ysa, LogOut );

//

if(Shuffle->value()){
  Out->add("\# Shuffling...");
  fann_shuffle_train_data(TrainData);
}

if(InitWghts->value()){
 
  Out->add("\# Initializing the weights using Widrow + Nguyen’s algorithm...");
  fann_init_weights(ysa,TrainData);
}

//
float err;
fann_set_train_stop_function(ysa,(fann_stopfunc_enum)StopFunction->value() ); 

sprintf(Buf,"\# Stop Function setted as  %s ",FANN_STOPFUNC_NAMES[StopFunction->value()]);
Out->add(Buf);
if(StopFunction->value()==0)
 err=DesiredError->value();
else
 err=BitFail->value();

SetFineTuning();
// inith Graph
cX.clear();
cY.clear();
//

if(ft.TestData && ft.overtraining)
  Out->add("Epoch       : Training MSE : Testing MSE  :  Bit Fail ");
else
  Out->add("Epoch       : Training MSE : Bit Fail ");


fann_train_on_data(ysa, TrainData, (int)MaxEpoch->value(),(int)EReports->value(),err );


if(stop){
   Out->add("@C1Process Stopped !");
   if(fl_choice("Training stopped !\\n  Do you want to save ANN file ?", "No","Yes",NULL))
      SaveANN(); 
}
else 
   SaveANN(); 

Refresh();
// fann_destroy(ysa);
DeactivateStop();} {}
  }
  Function {TrainCascade()} {} {
    code {if(TrainData==NULL){
  fl_alert("Firstly Load Train Data !");
  return;
}
if(working) return;
Out->clear();

ActivateStop();
Out->add("@C4Cascade Training...");
Refresh();
  ClearTrainingMemory();
double desired_error=DesiredError->value();
// unsigned int max_neurons = Input->value()+Hid1->value()+Hid2->value()+Hid3->value()+Output->value();
unsigned int max_neurons = (unsigned int )MaxCascade->value();
unsigned int neurons_between_reports = 1;

ysa = fann_create_shortcut(2, (int)Input->value(), (int)Output->value());
fann_set_training_algorithm(ysa,(fann_train_enum)Method->value());
fann_set_activation_function_hidden(ysa,(fann_activationfunc_enum)Act[HiddenActivationF->value()]);
fann_set_activation_function_output(ysa,(fann_activationfunc_enum)Act[OutputActivationF->value()]);
fann_set_callback(ysa, CascadeLogOut);
//
// fann_set_train_error_function(ysa, FANN_ERRORFUNC_LINEAR);
SetFineTuning();
SetCascadeTuning();
// init graph
cX.clear();
cY.clear(); 
//

if(ft.TestData && ft.overtraining)
  Out->add("Epoch       : Training MSE : Testing MSE  :  Bit Fail ");
else
  Out->add("Epoch       : Training MSE : Bit Fail ");
  

  
cascadeFirst=true;
fann_cascadetrain_on_data(ysa, TrainData, max_neurons, neurons_between_reports, desired_error);

//
if(stop){
   Out->add("@C1Process Stopped !");
   if(fl_choice("Training stopped !\\n  Do you want to save ANN file ?", "No","Yes",NULL))
      SaveANN(); 
}
else 
   SaveANN(); 

 // fann_destroy(ysa);
Refresh();
DeactivateStop();} {}
  }
  Function {Test()} {} {
    code {if(TestData==NULL){
  fl_alert("Firstly Load Test Data !");
  return;
}
if(working) return;
char Buf[512];
char* file=NULL;
Tabs->value(LogG);
Tabs->redraw();
Out->clear();
file = fl_file_chooser("Load Trained ANN", "*.net", file);
if(file!=NULL){
   Out->add("Trained ANN Loaded...");   
   ysa=fann_create_from_file(file);

   Out->add("@C4ANN Testing...");
   fann_reset_MSE(ysa);
   fann_test_data(ysa,TestData);

   sprintf(Buf,"Test Result MSE : %f ", fann_get_MSE(ysa) );
   Out->add(Buf);
 

}} {}
  }
  Function {Run()} {} {
    code {char Buf[512];
result=fann_run(ysa, data);

for(int i=0;i<ysa->num_input;i++){
  sprintf(Buf,"Input %03d : %f",i,data[i]);
  Out->add(Buf);

}

for(int i=0;i<ysa->num_output;i++){
  sprintf(Buf,"Output %03d : %f",i,result[i]);
  Out->add(Buf);

}} {}
  }
  Function {RunNormal()} {} {
    code {if(working) return;
char Buf[512];
char* file=NULL;
Out->clear();
file = fl_file_chooser("Load Trained ANN", "*.net", file);
if(file!=NULL){
   Out->add("Trained ANN Loaded...");   
   ysa=fann_create_from_file(file);

   Out->add("@C4Give Inputs...");
//

  


//
   GetInputs *op=new GetInputs();
   op->show(ysa);


 

}} {}
  }
  Function {RunwithFile()} {} {
    code {if(working) return;
char Buf[512];
char* file=NULL;
Out->clear();

if(TestData==NULL)
   LoadTestData();
file = fl_file_chooser("Load Trained ANN", "*.net", file);
if(file!=NULL){
   Out->add("Trained ANN Loaded...");
   ysa=fann_create_from_file(file);
   Out->add("@C4Runing with Testing Data File...");
   Out->add("Data No : Calculated Outputs : Real Outputs ");   


char Buf[512];
std::string ln1,ln2;

for(int i=0;i<TestData->num_data;i++){
  result=fann_run(ysa, TestData->input[i]);
  sprintf(Buf,"%04d : ",i+1);
//  Out->add(Buf);
  ln1=Buf;
  ln2="";
  for(int j=0;j<ysa->num_output;j++){
    sprintf(Buf,"%0.04f ",result[j]);
    ln1=ln1+Buf;
    sprintf(Buf,"%0.04f ",TestData->output[i][j]);
    ln2=ln2+Buf;

  }
  ln1=ln1+" : "+ln2;
  Out->add(ln1.c_str());

}

}} {}
  }
  Function {ClearLog()} {} {
    code {if(working) return;
Out->clear();} {}
  }
  Function {SaveLog()} {} {
    code {if(working) return;
if(Out->size()<1)return;
char Buf[512];
char* file=NULL;
file = fl_file_chooser("Save Log File", "*.log", file);
if(file!=NULL){
  FILE *fp;
  fp=fopen(file,"wt");
  if(fp){
   for(int i=1;i<=Out->size();i++){
     sprintf(Buf,"%s\\n",Out->text(i));
     fputs(Buf,fp);
   }
   fclose(fp);   

   Out->add("Log file Saved...");   
   
     
  }

}} {}
  }
  Function {LoadLog()} {} {
    code {if(working) return;
char* file=NULL;
Out->add("Select Log File...");
file = fl_file_chooser("Save Log File", "*.log", file);
if(file!=NULL)
   Out->load(file);} {}
  }
  Function {StopProcess()} {} {
    code {stop=true;
Out->add("@C1Stop pressed please wait ...");} {}
  }
  Function {About()} {} {
    code {AboutDlg *ab=new AboutDlg();
ab->show();} {}
  }
  Function {Help()} {} {
    code {} {}
  }
  Function {DataProcessing()} {open
  } {
    code {char *fn;
	char Buf [512];
	int num;

	if(working) return;

	fn=fl_file_chooser("Select Raw Data File ","*.txt", NULL, 0);


	if(fn){	
           int num=GetnItem(fn);
           if(num==1){
             tdp=new TimeSeri(); 
//             fl_alert("Time Series Raw Data File ");
             if(tdp->LoadRawData(fn)==false){
               fl_alert("Error readingg data File :(");
               return;           
             }               
	     TDPDlg *dp=new TDPDlg(fn);
	     dp->show();

	     return;           
           }
           else if (num>1){
             rdp=new DataProcess();
//             rdp->LoadRawData(fn);
             if(rdp->LoadRawData(fn)==false){
               fl_alert("Error readingg data File :(");
               return;           
             }               
	     NDPDlg *dp=new NDPDlg(fn);
	     dp->show();
// Ne uðraþtýrdý beni ...
//	     delete rdp;
	     return;                    

           }
           else{
             fl_alert("Error in Raw Data File !");
	     return;                      
           
           }            
 
   
	}
	else{

	  
	}} {selected
    }
  }
  Function {LoadTrainData()} {open
  } {
    code {char *fn;
char Buf [512];
int num,in,out;

if(working) return;

fn=fl_file_chooser("Select Train Data File ","*.dat", NULL, 0);


if(fn){
  if(TrainData)fann_destroy_train(TrainData);
  TrainData = fann_read_train_from_file(fn);
  int err=fann_get_errno( (struct fann_error *) ft.TrainData);
  if(err == FANN_E_NO_ERROR){
      Out->clear();
      Out->add("Training Data File Selected");           
      sprintf(Buf,"Number of Data : %d ",TrainData->num_data);
      Out->add(Buf);
      sprintf(Buf,"Number of Input : %d ",TrainData->num_input);
      Out->add(Buf);
      sprintf(Buf,"Number of Output : %d ",TrainData->num_output);
      Out->add(Buf);    
      Input->value(TrainData->num_input);
      Output->value(TrainData->num_output);   
      SetHiddens();       
      DataFile->value(fn);
      But_Alg->activate();
      But_Act->activate();
      But_Trn->activate();            
   
  }
  else{
   Out->add(fann_get_errstr((struct fann_error *)TrainData));
  
  }
  


}} {}
  }
  Function {LoadTestData()} {} {
    code {char *fn;
  char Buf [512];
  int num,in,out;

  if(working) return;

  fn=fl_file_chooser("Select Test Data File ","*.dat", NULL, 0);

  if(fn){
   // Eski Deðeri delete etme yok 
    if(TestData)fann_destroy_train(TestData);
    TestData = fann_read_train_from_file(fn);
    int err=fann_get_errno( (struct fann_error *) ft.TestData);
    if(err == FANN_E_NO_ERROR){
      Out->clear();
      Out->add("Test Data File Selected");           
      sprintf(Buf,"Number of Data : %d ",TestData->num_data);
      Out->add(Buf);
      sprintf(Buf,"Number of Input : %d ",TestData->num_input);
      Out->add(Buf);
      sprintf(Buf,"Number of Output : %d ",TestData->num_output);
      Out->add(Buf);    
      TestFile->value(fn);
      But_Test->activate();   
      OverTraining->activate();     
    }
    else{
      Out->add(fann_get_errstr((struct fann_error *)TestData));
  
    }

  }} {}
  }
  Function {SetFineTuning()} {} {
    code {fann_set_train_error_function(ysa,(fann_errorfunc_enum)ErrorFunction->value());
    fann_set_activation_steepness_hidden(ysa,(fann_type) HiddenStepness->value());
    fann_set_activation_steepness_output(ysa,(fann_type) OutputStepness->value());
    fann_set_quickprop_decay(ysa,DecayFactor->value());
    fann_set_quickprop_mu(ysa,MuFactor->value());
    fann_set_rprop_increase_factor(ysa,IncreaseFactor->value());
    fann_set_rprop_decrease_factor(ysa,DecreaseFactor->value());
    fann_set_rprop_delta_min(ysa,DeltaMin->value());
    fann_set_rprop_delta_max(ysa,DeltaMax->value());} {}
  }
  Function {SetCascadeTuning()} {} {
    code {fann_set_cascade_output_change_fraction(ysa,OutputChange->value());
    fann_set_cascade_output_stagnation_epochs(ysa,(unsigned int )OutputStag->value());
    fann_set_cascade_candidate_change_fraction(ysa,CandidateChange->value());
    fann_set_cascade_candidate_stagnation_epochs(ysa,  (unsigned  int)CandidateStag->value());
    fann_set_cascade_weight_multiplier(ysa,WeighMultiplier->value());
    fann_set_cascade_candidate_limit(ysa,(fann_type)CandidateLimit->value());
    fann_set_cascade_max_out_epochs(ysa,(unsigned  int)MaxOutEpoch->value());
    fann_set_cascade_max_cand_epochs(ysa,(unsigned  int)MaxCasndidatetEpoch->value());
    fann_set_cascade_num_candidate_groups(ysa,(unsigned  int)NumCandidateGroups->value());} {}
  }
  Function {NeuralNetworkInfo()} {} {
    code {if(working) return;
  char Buf[512];
  char* file=NULL;
  Out->clear();
  file = fl_file_chooser("Load Trained ANN", "*.net", file);
  if(file!=NULL){
     ysa=fann_create_from_file(file);
     Out->add("@C4Neural Network ");
     Out->add(file);
     Out->add("");
 //
     NetInfo *op=new NetInfo();
      op->show(ysa);


 

  }} {}
  }
  Function {CreateDesiredANN()} {private
  } {
    code {if(Layer->value()==5)
  ysa = fann_create_sparse(ConnectionRate->value(),5,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Hid3->value(),(int)Output->value());
else if(Layer->value()==4)
  ysa = fann_create_sparse(ConnectionRate->value(),4,(int)Input->value(),(int)Hid1->value(),(int)Hid2->value(),(int)Output->value());  
else
  ysa = fann_create_sparse(ConnectionRate->value(),3,(int)Input->value(),(int)Hid1->value(),(int)Output->value());

// Set Fine Tuning Parameters} {}
  }
  Function {ActivateStop()} {private
  } {
    code {stop=false;
Stop->activate();
working=true;} {}
  }
  Function {DeactivateStop()} {private
  } {
    code {stop=false;
Stop->deactivate();
working=false;} {}
  }
  Function {SaveANN()} {private
  } {
    code {SaveDlg *save=new SaveDlg();
save->show();} {}
  }
  Function {ClearTrainingMemory()} {private
  } {
    code {for(int i=0; i<4; i++) {
    MinTrainingMSE[i]=MinTestingMSE[i]=1;
  }} {}
  }
} 

class GetInputs {} {
  decl {struct fann *ann;} {public
  }
  decl {int ninputs, noutputs;} {public
  }
  Function {GetInputs()} {} {
    Fl_Window window {
      label {Run ANN} open
      xywh {405 393 610 328} type Double box PLASTIC_UP_BOX color 21 align 0 hide
      code0 {o->set_modal();}
    } {
      Fl_Scroll Grid {
        label Inputs open
        xywh {10 25 235 245} box DOWN_BOX when 1
      } {}
      Fl_Return_Button Ok {
        label Ok
        user_data {"GetInputs"}
        callback cb_Ok
        xywh {355 285 235 30} box PLASTIC_UP_BOX color 15
      }
      Fl_Scroll GridOut {
        label Outputs open
        xywh {355 25 235 245} box DOWN_BOX color 19 when 1
      } {}
      Fl_Button {} {
        label {Run @->}
        callback {char buf[10];
fann_type *data,*result;
data=new fann_type[ninputs];
Fl_Float_Input *f;

for(int i=0;i<ninputs;i++){
   f=(Fl_Float_Input *)Grid->child(i);
   data[i]=atof(f->value());
}

result=fann_run(ann, data);

for(int i=0;i<noutputs;i++){
   sprintf(buf,"%lf",result[i]);
   f=(Fl_Float_Input *)GridOut->child(i);
   f->value(buf);
}

Fl::focus(GridOut->child(0));
GridOut->end();
GridOut->redraw();}
        xywh {255 135 90 30} box PLASTIC_UP_BOX color 1
      }
    }
  }
  Function {show(struct fann *iann)} {} {
    code {ann=iann;
ninputs=fann_get_num_input(ann);
noutputs=fann_get_num_output(ann);

char *Num;
Fl_Float_Input *f;

// ft.data=new fann_type[ninputs];
window->show();
Grid->position(0,0);
Grid->begin();
for(int i=0;i<ninputs;i++){
  Num= new char[10];
  sprintf(Num," %03d : ",i+1);
// SCROLL ÝÇÝN EK YAPARKEN SCROLLUN POZÝSYONU ÜZERÝNDEN EKLE  !!!
  f=new Fl_Float_Input(Grid->x()+70,Grid->y()+20*i+3,70,20,Num);
}


GridOut->position(0,0);
GridOut->begin();
for(int i=0;i<noutputs;i++){
  Num= new char[10];
  sprintf(Num," %03d : ",i+1);
// SCROLL ÝÇÝN EK YAPARKEN SCROLLUN POZÝSYONU ÜZERÝNDEN EKLE  !!!
  f=new Fl_Float_Input(GridOut->x()+70,GridOut->y()+20*i+3,70,20,Num);
}
Fl::focus(GridOut->child(0));
GridOut->end();
GridOut->redraw();

Fl::focus(Grid->child(0));
Grid->end();
Grid->redraw();} {}
  }
  Function {hide()} {} {
    code {window->hide();} {}
  }
} 

class NetInfo {} {
  decl {vector<double> weights;} {}
  decl {struct fann *ann;} {public
  }
  Function {NetInfo()} {open
  } {
    Fl_Window window {
      label {Neural Network Information} open
      xywh {429 144 533 374} type Double box PLASTIC_UP_BOX color 21 align 0 hide
      code0 {o->set_modal();}
    } {
      Fl_Return_Button Ok {
        label Ok
        user_data {"GetInputs"}
        callback cb_Ok
        xywh {362 330 161 30} box PLASTIC_UP_BOX color 15
      }
      Fl_Value_Output Input {
        label {Input Layer  + 1 Bias}
        xywh {50 56 50 24} box PLASTIC_UP_BOX color 140 align 1
      }
      Fl_Browser Hidden {
        label {Hidden Layers}
        xywh {160 31 242 71} box PLASTIC_UP_BOX color 140 align 1
      }
      Fl_Box {} {
        label {@+4->}
        xywh {115 53 35 27} labelcolor 15
      }
      Fl_Box {} {
        label {@+4->}
        xywh {415 53 30 27} labelcolor 15
      }
      Fl_Value_Output Output {
        label {Output Layer}
        xywh {460 51 50 24} box PLASTIC_UP_BOX color 140 align 1
      }
      Fl_Output NType {
        label {Network Type :}
        xywh {150 131 290 24}
      }
      Fl_Output Alg {
        label {Training Algorithm :}
        xywh {150 165 290 24}
      }
      Fl_Output Err {
        label {Error Function :}
        xywh {150 206 290 24} hotspot
      }
      Fl_Output Stop {
        label {Stop Function :}
        xywh {150 240 290 24} hotspot
      }
      Fl_Choice WLabel {
        label {Weights : }
        callback {WValue->value(weights[WLabel->value()]);} open
        xywh {77 285 243 25} down_box BORDER_BOX
      } {}
      Fl_Value_Output WValue {
        label {@+4->}
        xywh {350 286 115 24} color 7 labelcolor 15
      }
    }
  }
  Function {show(struct fann *iann)} {open
  } {
    code {ann=iann;

  char buf[512];
  int j=1;
  struct fann_layer *layer_it;
  
  Input->value( ann->num_input );
  Output->value( ann->num_output );
  
  sprintf(buf, "Input layer                          :%4d neurons, 1 bias\\n", ann->num_input);
  Out->add(buf);
  for(layer_it = ann->first_layer + 1; layer_it != ann->last_layer - 1; layer_it++)
  {
    if(ann->network_type == FANN_NETTYPE_SHORTCUT){
      sprintf(buf,"  Hidden layer  %d                    :%4d neurons, 0 bias\\n",j,
				   layer_it->last_neuron - layer_it->first_neuron);
      Out->add(buf);	
      sprintf(buf," Layer %d : %4d neurons, 0 bias\\n",j,
				   layer_it->last_neuron - layer_it->first_neuron);      
      Hidden->add(buf);			   
    }
    else{
      sprintf(buf,"  Hidden layer  %d                    :%4d neurons, 1 bias\\n",j,
				   layer_it->last_neuron - layer_it->first_neuron - 1);
      Out->add(buf);
      sprintf(buf," Layer %d : %4d neurons, 1 bias\\n",j,
				   layer_it->last_neuron - layer_it->first_neuron);            
      Hidden->add(buf);			         
    }
    j++;
  }
  sprintf(buf,"Output layer                         :%4d neurons\\n", ann->num_output);
  Out->add(buf);
  sprintf(buf,"Total neurons and biases             :%4d\\n", fann_get_total_neurons(ann));
  Out->add(buf);  
  sprintf(buf,"Total connections                    :%4d\\n", ann->total_connections);
  Out->add(buf);  
  sprintf(buf,"Connection rate                      :%8.3f\\n", ann->connection_rate);
  Out->add(buf);  
  sprintf(buf,"Network type                         :   %s\\n", FANN_NETTYPE_NAMES[ann->network_type]);  
  Out->add(buf);  
  NType->value(FANN_NETTYPE_NAMES[ann->network_type]);
  sprintf(buf,"Training algorithm                   :   %s\\n", FANN_TRAIN_NAMES[ann->training_algorithm]);
  Out->add(buf);  
  Alg->value(FANN_TRAIN_NAMES[ann->training_algorithm]);
  sprintf(buf,"Training error function              :   %s\\n", FANN_ERRORFUNC_NAMES[ann->train_error_function]);
  Out->add(buf);  
  Err->value(FANN_ERRORFUNC_NAMES[ann->train_error_function]);
  sprintf(buf,"Training stop function               :   %s\\n", FANN_STOPFUNC_NAMES[ann->train_stop_function]);
  Stop->value(FANN_STOPFUNC_NAMES[ann->train_stop_function]);
  Out->add(buf);  
  sprintf(buf,"Bit fail limit                       :%8.3f\\n", ann->bit_fail_limit);
  Out->add(buf);  
  sprintf(buf,"Learning rate                        :%8.3f\\n", ann->learning_rate);
  Out->add(buf);  
  sprintf(buf,"Learning momentum                    :%8.3f\\n", ann->learning_momentum);
  Out->add(buf);  
  sprintf(buf,"Quickprop decay                      :%11.6f\\n", ann->quickprop_decay);
  Out->add(buf);  
  sprintf(buf,"Quickprop mu                         :%8.3f\\n", ann->quickprop_mu);
  Out->add(buf);  
  sprintf(buf,"RPROP increase factor                :%8.3f\\n", ann->rprop_increase_factor);
  Out->add(buf);  
  sprintf(buf,"RPROP decrease factor                :%8.3f\\n", ann->rprop_decrease_factor);
  Out->add(buf);  
  sprintf(buf,"RPROP delta min                      :%8.3f\\n", ann->rprop_delta_min);
  Out->add(buf);  
  sprintf(buf,"RPROP delta max                      :%8.3f\\n", ann->rprop_delta_max);
  Out->add(buf);  
  sprintf(buf,"Cascade output change fraction       :%11.6f\\n", ann->cascade_output_change_fraction);
  Out->add(buf);  
  sprintf(buf,"Cascade candidate change fraction    :%11.6f\\n", ann->cascade_candidate_change_fraction);
  Out->add(buf);  
  sprintf(buf,"Cascade output stagnation epochs     :%4d\\n", ann->cascade_output_stagnation_epochs);
  Out->add(buf);  
  sprintf(buf,"Cascade candidate stagnation epochs  :%4d\\n", ann->cascade_candidate_stagnation_epochs);
  Out->add(buf);  
  sprintf(buf,"Cascade max output epochs            :%4d\\n", ann->cascade_max_out_epochs);
  Out->add(buf);  
  sprintf(buf,"Cascade max candidate epochs         :%4d\\n", ann->cascade_max_cand_epochs);
  Out->add(buf);  
  sprintf(buf,"Cascade weight multiplier            :%8.3f\\n", ann->cascade_weight_multiplier);
  Out->add(buf);  
  sprintf(buf,"Cascade candidate limit              :%8.3f\\n", ann->cascade_candidate_limit);
  Out->add(buf);  
  for(unsigned int i = 0; i < ann->cascade_activation_functions_count; i++){
    sprintf(buf,"Cascade activation functions[%d]      :   %s\\n", i,
			FANN_ACTIVATIONFUNC_NAMES[ann->cascade_activation_functions[i]]);
    Out->add(buf);			
   }
   for(unsigned int i = 0; i < ann->cascade_activation_steepnesses_count; i++){
     sprintf(buf,"Cascade activation steepnesses[%d]    :%8.3f\\n", i,
			ann->cascade_activation_steepnesses[i]);
     Out->add(buf);			
   }
   
   sprintf(buf,"Cascade candidate groups             :%4d\\n", ann->cascade_num_candidate_groups);
   Out->add(buf);   
   sprintf(buf,"Cascade no. of candidates            :%4d\\n", fann_get_cascade_num_candidates(ann));
   Out->add(buf);   
 //
    struct fann_neuron *first_neuron;
    struct fann_neuron *neuron_it;
    unsigned int index;
    unsigned int source_index;
    unsigned int destination_index;
    int lyr=0;

    first_neuron = ann->first_layer->first_neuron;

    source_index = 0;
    destination_index = 0;

    /* The following assumes that the last unused bias has no connections */

    /* for each layer */
    for(layer_it = ann->first_layer; layer_it != ann->last_layer; layer_it++,lyr++){
      sprintf(buf,"Layer %d ",lyr);
      Out->add(buf);
        /* for each neuron */
        for(neuron_it = layer_it->first_neuron; neuron_it != layer_it->last_neuron; neuron_it++){
            /* for each connection */
            for (index = neuron_it->first_con; index < neuron_it->last_con; index++){
                sprintf(buf," From Neuron To Neuron  ( %d ---> %d ) :  Weight : %lf"
                    ,(ann->connections[source_index] - first_neuron),destination_index,ann->weights[source_index]);
                Out->add(buf);
                sprintf(buf,"From Neuron  %d To Neuron %d ",(ann->connections[source_index] - first_neuron),destination_index);
                WLabel->add(buf);
                weights.push_back(ann->weights[source_index]);
                source_index++;
            }
            destination_index++;
        }
    }
    WLabel->value(0);
    WValue->value(weights[0]);

   //   
   
   
   
   window->show();} {}
  }
  Function {hide()} {} {
    code {window->hide();} {}
  }
} 

class AboutDlg {} {
  Function {AboutDlg()} {} {
    Fl_Window window {
      label About open
      xywh {284 125 400 315} type Double box PLASTIC_UP_BOX color 22 align 0 hide
      code0 {o->set_modal();} modal
    } {
      Fl_Return_Button Ok {
        label Ok
        callback cb_Ok
        xywh {264 273 130 30} box PLASTIC_UP_BOX color 15
      }
      Fl_Group {} {
        xywh {8 8 233 75} box EMBOSSED_FRAME color 1
      } {
        Fl_Box {} {
          image {img/logo4.png} xywh {10 10 230 70} box SHADOW_FRAME color 1 align 64
        }
      }
      Fl_Browser Info {
        xywh {10 87 380 176} color 31 textcolor 4
      }
    }
  }
  Function {show()} {} {
    code {Info->add("* FannTool is a GUI tool for ANN by using FANN library");
Info->add("  FANN is a free open source neural network library");
Info->add("  FANN Web Page : leenissen.dk/fann/");
Info->add("  FannTool page : http://code.google.com/p/fanntool/");
Info->add("* Programmed by BlueKid");
Info->add("  http://derindelimavi.blogspot.com/");
Info->add("* Please Send me any suggestion, modification or bugs.");
Info->add(" Don't hesitate to contact me for any question");
Info->add(" I will be very grateful with your feedbacks.");
Info->add(" bluekid70@gmail.com");

window->show();} {}
  }
  Function {hide()} {} {
    code {window->hide();} {}
  }
} 

class NDPDlg {} {
  Function {NDPDlg(char * FileName)} {} {
    Fl_Window window {
      label {Raw Data Processsing } open
      xywh {263 120 755 533} type Double box PLASTIC_UP_BOX color 19 align 0 hide resizable
      code0 {o->set_modal();} modal
    } {
      Fl_Return_Button Ok {
        label Exit
        callback {// cb_Ok
delete rdp;
((Fl_Window *)(o->parent()))->hide();}
        xywh {619 497 130 30} box PLASTIC_UP_BOX color 15
      }
      Fl_Output RawFile {
        label {Raw Data File :}
        xywh {110 14 367 24}
        code0 {RawFile->value(FileName);}
      }
      Fl_Value_Output nDataPoint {
        label {Number of Data:}
        xywh {599 15 80 24}
      }
      Fl_Value_Output nInput {
        label {Number of Input :}
        xywh {128 50 44 24}
      }
      Fl_Counter nOutput {
        label {Number of output :}
        callback {nInput->value(rdp->GetNItem()-nOutput->value());}
        xywh {310 48 75 26} type Simple box GTK_UP_BOX color 33 align 4 minimum 1 step 1 value 1
      }
      Fl_Button Wbt {
        label {Write Data Files}
        callback {bool result;
if(shuffle->value())
        rdp->Shuffle();
  if(Scale_D->value()){
     result=rdp->WriteScaleParameters((char*)fName->value());    
     rdp->ScaleAll();
     if(result)
       fl_message("Scaling Parameters saved successfully ");
     else
       fl_alert("Error occured during Scaling Parameters saving ");     
     
     
   }  
  
  result=rdp->WriteData((char*)fName->value(),nOutput->value(),rTrain->value());
  
  if(result)
    fl_message("Data Files saved successfully ");
  else
    fl_alert("Error occured during Data Files saving ");
    
delete rdp;
((Fl_Window *)(o->parent()))->hide();}
        xywh {472 499 130 30} box PLASTIC_UP_BOX color 10
      }
      Fl_Value_Slider rTrain {
        label {Ratio of Training}
        callback {rTest->value(1.0-rTrain->value());}
        xywh {20 100 175 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 5 minimum 0.5 step 0.1 value 0.6 textsize 14
      }
      Fl_Value_Slider rTest {
        label {Ratio of Testing}
        callback {rTrain->value(1.0-rTest->value());}
        xywh {210 100 175 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 5 maximum 0.5 step 0.1 value 0.4 textsize 14
      }
      Fl_Check_Button shuffle {
        label {Shuffle Data}
        xywh {440 54 85 20} down_box DOWN_BOX
      }
      Fl_Input fName {
        label {Data File Name :}
        xywh {560 96 136 24}
        code0 {fName->value("Out");}
      }
      Fl_Check_Button Scale_D {
        label {Scale Data}
        xywh {570 50 120 24} down_box DOWN_BOX value 1
      }
      Fl_Group {} {open
        xywh {15 130 735 365} box PLASTIC_UP_BOX color 19
      } {
        Fl_Counter Item {
          label {Item No}
          callback {GetItemData();}
          xywh {100 154 56 20} type Simple labelcolor 4 align 4 minimum 0 step 1
        }
        Fl_Value_Output Min {
          xywh {40 453 90 25} box PLASTIC_UP_BOX color 19
        }
        Fl_Value_Output Max {
          xywh {655 451 80 25} box PLASTIC_UP_BOX color 19 align 16
        }
        Fl_Box Chart {
          tooltip Histogram xywh {40 179 695 268} box EMBOSSED_BOX color 34
          code0 {Chart->type(FL_BAR_CHART);}
          code1 {Chart->bounds(-0.05,1);}
          class Fl_Chart
        }
        Fl_Value_Slider MinV {
          label {Minimum Output Value}
          callback {itmp=rdp->GetItem(Item->value());
  itmp->SetMinV(MinV->value());}
          xywh {174 154 142 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 1 minimum -1 maximum 0.3 step 0.1 textsize 14
        }
        Fl_Value_Slider MaxV {
          label {Maximum Output Value}
          callback {itmp=rdp->GetItem(Item->value());
  itmp->SetMaxV(MaxV->value());}
          xywh {329 154 146 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 1 minimum 0.5 step 0.1 value 1 textsize 14
        }
        Fl_Check_Button Scale_I {
          label {Scale Item }
          callback {itmp=rdp->GetItem(Item->value());
  itmp->scale=Scale_I->value();}
          xywh {514 154 64 15} down_box DOWN_BOX value 1
        }
      }
    }
  }
  Function {show()} {} {
    code {nDataPoint->value(rdp->GetNData());
  nInput->value(rdp->GetNItem()-1);
  nOutput->maximum(rdp->GetNItem()-1);
  Item->maximum(rdp->GetNItem()-1);  
  GetItemData();  
  window->show();} {}
  }
  Function {hide()} {} {
    code {window->hide();} {}
  }
  Function {GetItemData()} {} {
    code {//  Item kýsmý 
  itmp=rdp->GetItem(Item->value());
  Min->value(itmp->GetMin());
  MinV->value(itmp->GetMinV());  
  Max->value(itmp->GetMax());  
  MaxV->value(itmp->GetMaxV());    
  Chart->clear();
  double *hst=itmp->Histogram(10);
  for(int i=0; i<10;i++){
    char buf[50];
    sprintf(buf,"%.1f",100*hst[i]);   
    Chart->add(hst[i],buf,19);
  }
  Scale_I->value(itmp->scale);} {}
  }
} 

class SaveDlg {} {
  Function {SaveDlg()} {} {
    Fl_Window window {
      label {Save ANN} open
      xywh {554 28 573 366} type Double box PLASTIC_UP_BOX color 22 align 0 hide resizable
      code0 {o->set_modal();} modal
    } {
      Fl_Return_Button Ok {
        label Exit
        callback cb_Ok
        xywh {424 315 130 30} box PLASTIC_UP_BOX color 15
      }
      Fl_Group {} {open
        xywh {20 27 535 238} box EMBOSSED_FRAME
      } {
        Fl_Group {} {open
          xywh {45 82 170 140}
        } {
          Fl_Round_Button Latest {
            label {Latest  ANN}
            xywh {45 82 170 20} type Radio down_box ROUND_DOWN_BOX value 1
          }
          Fl_Round_Button ANN0 {
            label {Minimum Training MSE}
            xywh {45 115 170 24} type Radio down_box ROUND_DOWN_BOX
          }
          Fl_Round_Button ANN1 {
            label {Minimum Testing MSE}
            xywh {45 163 170 19} type Radio down_box ROUND_DOWN_BOX deactivate
          }
          Fl_Round_Button ANN2 {
            label {Minimum  OCS MSE}
            xywh {45 202 170 20} type Radio down_box ROUND_DOWN_BOX deactivate
          }
        }
        Fl_Value_Output {} {
          label {Training MSE }
          xywh {295 78 100 24} align 1
          code0 {o->value(MinTrainingMSE[3]);}
        }
        Fl_Group {} {open
          xywh {295 78 230 145}
        } {
          Fl_Group OCS1 {open
            xywh {420 78 105 61} deactivate
          } {
            Fl_Value_Output {} {
              label {Testing MSE }
              xywh {420 78 105 24} align 1
              code0 {if(ft.TestData && ft.overtraining) o->value(MinTestingMSE[3]);}
            }
            Fl_Value_Output {} {
              xywh {420 115 105 24} align 1
              code0 {if(ft.TestData && ft.overtraining)o->value(MinTestingMSE[0]);}
            }
          }
          Fl_Group OCS2 {open
            xywh {295 159 230 64} deactivate
          } {
            Fl_Value_Output {} {
              xywh {295 159 100 24} align 1
              code0 {if(ft.TestData && ft.overtraining) o->value(MinTrainingMSE[1]);}
            }
            Fl_Value_Output {} {
              xywh {420 159 105 24} align 1
              code0 {if(ft.TestData && ft.overtraining)o->value(MinTestingMSE[1]);}
            }
            Fl_Value_Output {} {
              xywh {295 198 100 24} align 1
              code0 {if(ft.TestData && ft.overtraining) o->value(MinTrainingMSE[2]);}
            }
            Fl_Value_Output {} {
              xywh {420 198 105 24} align 1
              code0 {if(ft.TestData && ft.overtraining)o->value(MinTestingMSE[2]);}
            }
          }
        }
        Fl_Value_Output {} {
          xywh {295 115 100 24} align 1
          code0 {o->value(MinTrainingMSE[0]);}
        }
      }
      Fl_Button {} {
        label {&Save ANN}
        callback {char* file=NULL;
int i=-1;
if(Latest->value()){
  i=3;
}
else if(ANN0->value())
  i=0;
else if(ANN1->value())
  i=1;
else if(ANN2->value())
  i=2;
  
file = fl_file_chooser("Save Selected ANN", "*.net", file);
if(file!=NULL){
   fann_save(MinANN[i], file);
   fl_message("ANN Saved...");   
}}
        xywh {250 315 130 30} box PLASTIC_UP_BOX shortcut 0x40073 color 1
      }
    }
  }
  Function {show()} {} {
    code {if(ft.TestData && ft.overtraining){
  ANN1->activate();
  ANN2->activate();
  OCS1->activate();
  OCS2->activate();
  
}
window->show();} {}
  }
  Function {hide()} {} {
    code {window->hide();} {}
  }
  Function {~SaveDlg()} {} {
    code {fl_alert("Save dialog Closing");} {}
  }
} 

class TDPDlg {} {
  Function {TDPDlg(char * FileName)} {open
  } {
    Fl_Window window {
      label {Time Series  Data Processsing } open
      xywh {271 180 784 530} type Double box PLASTIC_UP_BOX color 19 align 0 hide resizable
      code0 {o->set_modal();} modal
    } {
      Fl_Return_Button Ok {
        label Exit
        callback {// cb_Ok
delete rdp;
((Fl_Window *)(o->parent()))->hide();}
        xywh {657 495 130 30} box PLASTIC_UP_BOX color 15
      }
      Fl_Output RawFile {
        label {Raw Data File :}
        xywh {115 16 395 24}
        code0 {RawFile->value(FileName);}
      }
      Fl_Value_Output nDataPoint {
        label {Number of Data:}
        xywh {665 16 80 24}
      }
      Fl_Counter nInput {
        label {Number of Intput :}
        callback {tdp->nInput=nInput->value();}
        xywh {135 48 75 26} type Simple box GTK_UP_BOX color 33 align 4 minimum 1 step 1 value 1
      }
      Fl_Button Wbt {
        label {Write Data Files}
        callback {bool result;
// if(shuffle->value()) tdp->Shuffle();
  if(Scale_D->value()){
     result=tdp->WriteScaleParameters((char*)fName->value());    
     if(tdp->scale) tdp->Scale();
     if(result)
       fl_message("Scaling Parameters saved successfully ");
     else
       fl_alert("Error occured during Scaling Parameters saving ");     
     
     
   }  
  
  result=tdp->WriteData((char*)fName->value(),rTrain->value());
  
  if(result)
    fl_message("Data Files saved successfully ");
  else
    fl_alert("Error occured during Data Files saving ");
    
delete rdp;
((Fl_Window *)(o->parent()))->hide();}
        xywh {505 495 130 30} box PLASTIC_UP_BOX color 10
      }
      Fl_Value_Slider rTrain {
        label {Ratio of Training}
        callback {rTest->value(1.0-rTrain->value());}
        xywh {20 100 175 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 5 minimum 0.5 step 0.1 value 0.6 textsize 14
      }
      Fl_Value_Slider rTest {
        label {Ratio of Testing}
        callback {rTrain->value(1.0-rTest->value());}
        xywh {230 100 175 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 5 maximum 0.5 step 0.1 value 0.4 textsize 14
      }
      Fl_Check_Button Shuffle {
        label {Shuffle Data}
        callback {tdp->shuffle=Shuffle->value();}
        xywh {440 54 85 20} down_box DOWN_BOX
      }
      Fl_Input fName {
        label {Data File Name :}
        xywh {560 96 136 24}
        code0 {fName->value("Out");}
      }
      Fl_Check_Button Scale_D {
        label {Scale Data}
        callback {tdp->scale=Scale_D->value();}
        xywh {570 50 120 24} down_box DOWN_BOX value 1
      }
      Fl_Group Gr {open
        xywh {15 130 767 355} box PLASTIC_UP_BOX color 19
      } {
        Fl_Value_Output Min {
          xywh {35 446 75 24} box PLASTIC_UP_BOX color 19
        }
        Fl_Value_Output Max {
          xywh {692 451 80 24} box PLASTIC_UP_BOX color 19 align 16
        }
        Fl_Box Chart {
          xywh {40 179 710 268} box EMBOSSED_BOX color 34
          code0 {Chart->type(FL_BAR_CHART);}
          code1 {Chart->bounds(-0.05,1);}
          class Fl_Chart
        }
        Fl_Value_Slider MinV {
          label {Minimum Output Value}
          callback {tdp->SetMinV(MinV->value());}
          xywh {55 149 142 20} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 1 minimum -1 maximum 0.3 step 0.1 textsize 14
        }
        Fl_Value_Slider MaxV {
          label {Maximum Output Value}
          callback {tdp->SetMaxV(MaxV->value());}
          xywh {225 151 146 18} type Horizontal box EMBOSSED_BOX color 22 selection_color 1 align 1 minimum 0.5 step 0.1 value 1 textsize 14
        }
        Fl_Group {} {open
          xywh {395 140 355 29} color 23
        } {
          Fl_Light_Button Hist {
            label Histogram
            callback {DrawGraph();}
            xywh {425 145 115 20} type Radio box NO_BOX value 1 selection_color 1
          }
          Fl_Light_Button {} {
            label {Time Series}
            callback {DrawGraph();}
            xywh {590 145 115 20} type Radio box NO_BOX selection_color 1
          }
        }
      }
      Fl_Value_Output {} {
        label {Number of output:}
        xywh {365 50 25 24} value 1
      }
    }
  }
  Function {show()} {open
  } {
    code {nDataPoint->value(tdp->GetNData());
  nInput->value(tdp->nInput);
//  nOutput->maximum(tdp->nOutput);

//  itmp=rdp->GetItem(Item->value());
  Min->value(tdp->GetMin());
  MinV->value(tdp->GetMinV());  
  Max->value(tdp->GetMax());  
  MaxV->value(tdp->GetMaxV());    
  DrawGraph();
  Scale_D->value(tdp->scale);


  window->show();} {}
  }
  Function {hide()} {} {
    code {window->hide();} {}
  }
  Function {DrawGraph()} {open
  } {
    code {if(Hist->value()){
    Chart->clear();
    Chart->type(FL_BAR_CHART);
    Chart->bounds(-0.05,1);    
    Chart->resize(40,179,710,268);    
    Max->position(692,451);    
    double *hst=tdp->Histogram(10);
    for(int i=0; i<10;i++){
      char buf[50];
      sprintf(buf,"%.1f",100*hst[i]);   
      Chart->add(hst[i],buf,19);
    }
  }
  else{
    Chart->clear();
    Chart->type(FL_LINE_CHART);
    Chart->bounds(Min->value(),Max->value());  
    Chart->resize(115,179,635,268);
    Max->position(30,183);        
    for(long i=0; i<tdp->count;i++){
       Chart->add(tdp->GetData(i));
    }
  
  }
  
Gr->redraw();} {}
  }
} 
